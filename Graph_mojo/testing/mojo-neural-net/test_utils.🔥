from testing import assert_equal, assert_true, assert_false
from tensor import Tensor, TensorSpec, TensorShape
from sys.info import simdwidthof, simdbitwidth
from algorithm import parallelize, vectorize
from utils.index import Index, IndexList
from random import randn, random_si64, seed
from pathlib import path
from python import Python, PythonObject
from math import exp
from NN_Utils import NN_Utils
from Data_Loader import MNIST_Data_Loader_Pytorch
from CustomNN import Custom_NN_Pytorch, Custom_NN_Mojo

alias type = DType.float64
alias simdwidth = simdwidthof[type]()
alias mu: Float64 = 0.01
alias beta1: Float64 = 0.9
alias beta2: Float64 = 0.99
alias epsilon: Float64 = 0.00000001
alias mini_batch_size: Int = 50
alias epochs: Int = 1
alias error_target: Float32 = .1
alias input_layer_size: Int = 16
alias hidden_layer_size: Int = 52
alias output_layer_size: Int = 26
alias data_width: Int = 17
alias data_size: Int = 20000
alias training_size: Int = 16000
alias validation_size: Int = 4000


struct Mojo_Tensor_Index:
    var tensor_shape_list: List[Int]
    var tensor_index_list: List[Int]
    
    fn __init__(mut self, tensor_shape_list: List[Int]):
        self.tensor_shape_list = tensor_shape_list
        self.tensor_index_list = List[Int]()
        
        # Init tensor_index_list
        for dim in range(len(tensor_shape_list)):
            self.tensor_index_list.append(0)
     
    fn begin(mut self):
        for dim in range(len(self.tensor_index_list)):
            self.tensor_index_list[dim] = 0
            
    fn end(mut self) -> Bool:        
        for dim in range(len(self.tensor_index_list)):
            if self.tensor_index_list[dim] < self.tensor_shape_list[dim] - 1:
                return False
        if self.tensor_index_list[-1] < self.tensor_shape_list[-1]:
            return False
        return True
                     
    fn inc_mojo_tensor_index(mut self):
            
        var i: Int = len(self.tensor_index_list) - 1
        self.tensor_index_list[-1] = self.tensor_index_list[-1] + 1
        
        if self.end():
            return
        
        while i > 0 and self.tensor_index_list[i] >= self.tensor_shape_list[i]:
            self.tensor_index_list[i] = 0
            self.tensor_index_list[i-1] = self.tensor_index_list[i-1] + 1
            i = i - 1
    
    fn print_mojo_tensor_index_list(mut self):
        for i in range(len(self.tensor_index_list)):
            print(self.tensor_index_list[i], end=" ")
        print("")
        
    fn print_mojo_tensor_shape_list(mut self):
        for i in range(len(self.tensor_shape_list)):
            print(self.tensor_shape_list[i], end=" ")
        print("")


fn create_equivalent_mojo_tensor(numpy_array: PythonObject) raises -> Tensor[type]:
    np = Python.import_module("numpy")
    
    # Get Shape of Tensor
    var tensor_shape_list = List[Int]()
    for d in numpy_array.shape:
        tensor_shape_list.append(d)
    
    # Construct Tensor
    var mojo_tensor = Tensor[type](TensorShape(tensor_shape_list))
        
    var tensor_index: Mojo_Tensor_Index = Mojo_Tensor_Index(tensor_shape_list)
        
    if len(tensor_index.tensor_shape_list) == 1:   
        while tensor_index.end() == False:
            mojo_tensor[Index(tensor_index.tensor_index_list[0])] = Float64(numpy_array[tensor_index.tensor_index_list[0]])
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 2:   
        while tensor_index.end() == False:
            mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1])] = Float64(numpy_array[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1]])
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 3:   
        while tensor_index.end() == False:
            mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2])] = Float64(numpy_array[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2]])
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 4:   
        while tensor_index.end() == False:
            mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3])] = Float64(numpy_array[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3]])
            tensor_index.inc_mojo_tensor_index()            
    
    return mojo_tensor
    
fn create_equivalent_pytorch_tensor(mojo_tensor: Tensor[type]) raises -> PythonObject: 
    torch = Python.import_module("torch")
    
    # Get Shape of Tensor
    var tensor_shape: PythonObject = Python.list()
    var tensor_shape_list = List[Int]()
    
    for dim in range(mojo_tensor.rank()):
        tensor_shape.append(mojo_tensor.shape()[dim])
        tensor_shape_list.append(dim)
        
    # Construct Tensor
    var pytorch_tensor: PythonObject = torch.zeros(size=tensor_shape)
    
    var tensor_index: Mojo_Tensor_Index = Mojo_Tensor_Index(tensor_shape_list)
        
    if len(tensor_index.tensor_shape_list) == 1:   
        while tensor_index.end() == False:
            pytorch_tensor[tensor_index.tensor_index_list[0]] = mojo_tensor[Index(tensor_index.tensor_index_list[0])]
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 2:   
        while tensor_index.end() == False:
            pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1]] = mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1])]
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 3:   
        while tensor_index.end() == False:
            pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2]] = mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2])]
            tensor_index.inc_mojo_tensor_index()
    elif len(tensor_index.tensor_shape_list) == 4:   
        while tensor_index.end() == False:
            pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3]] = mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3])]
            tensor_index.inc_mojo_tensor_index()  
    
    return pytorch_tensor

fn mojo_tensor_equals_pytroch_tensor(mojo_tensor: Tensor[type], pytorch_tensor: PythonObject) raises -> Bool:
    var good: Bool = False
    
    if len(pytorch_tensor.shape) == mojo_tensor.shape().rank():
        good = True
        var pytorch_tensor_shape: PythonObject = pytorch_tensor.shape
        for d in range(len(pytorch_tensor.shape)):
            if pytorch_tensor.shape[d] != mojo_tensor.shape()[d]:
                return False   
        var precision: Float64 = 0.0001
        var tensor_shape_list = List[Int]()

        for dim in range(mojo_tensor.rank()):
            tensor_shape_list.append(dim)
        
        var tensor_index: Mojo_Tensor_Index = Mojo_Tensor_Index(tensor_shape_list)
            
        if len(tensor_index.tensor_shape_list) == 1:   
            while tensor_index.end() == False:
                var pytorch_val = Float64(pytorch_tensor[tensor_index.tensor_index_list[0]].item())
                var mojo_val = Float64(mojo_tensor[Index(tensor_index.tensor_index_list[0])])                 
                if (Float64(pytorch_val) > (mojo_val + precision)) \
                or (Float64(pytorch_val) < (mojo_val - precision)):
                    return False   
                tensor_index.inc_mojo_tensor_index()
        elif len(tensor_index.tensor_shape_list) == 2:   
            while tensor_index.end() == False:
                var pytorch_val = Float64(pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1]].item())
                var mojo_val = Float64(mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1])])                
                if (Float64(pytorch_val) > (mojo_val + precision)) \
                or (Float64(pytorch_val) < (mojo_val - precision)):
                    return False   
                tensor_index.inc_mojo_tensor_index()
        elif len(tensor_index.tensor_shape_list) == 3:   
            while tensor_index.end() == False:
                var pytorch_val = Float64(pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2]].item())
                var mojo_val = Float64(mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2])])                
                if (Float64(pytorch_val) > (mojo_val + precision)) \
                or (Float64(pytorch_val) < (mojo_val - precision)):
                    return False   
                tensor_index.inc_mojo_tensor_index()
        elif len(tensor_index.tensor_shape_list) == 4:   
            while tensor_index.end() == False:
                var pytorch_val = Float64(pytorch_tensor[tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3]].item())
                var mojo_val = Float64(mojo_tensor[Index(tensor_index.tensor_index_list[0], tensor_index.tensor_index_list[1], tensor_index.tensor_index_list[2], tensor_index.tensor_index_list[3])])                
                if (Float64(pytorch_val) > (mojo_val + precision)) \
                or (Float64(pytorch_val) < (mojo_val - precision)):
                    return False   
                tensor_index.inc_mojo_tensor_index()

    return good
    
    
    
def test_execution():
    # Just tests running a mojo test
    assert_equal(0, 0)    
    
def test_sigmoid():
    torch = Python.import_module("torch")
    nn = Python.import_module("torch.nn")
    np = Python.import_module("numpy")
    var sigmoid_pytorch = nn.Sigmoid()

    var tests_passed: Bool = False
    var numpy_array = np.random.uniform(-10, 10, size=(1024,128))
    var mojo_tensor = create_equivalent_mojo_tensor(numpy_array)  
    var pytorch_tensor: PythonObject = torch.tensor(numpy_array)
    
    var nn_utils = NN_Utils()
    
    print("Running Sigmoid Tests...")
    print("--------------------------------------------------")
            
    var time = Python.import_module("time")

    var tm1 = time.time()        
    pytorch_tensor = sigmoid_pytorch(pytorch_tensor)
    var dur1 = time.time()-tm1
    print("Pytorch Sigmoid:",dur1,"seconds")
    
    var tm2 = time.time() 
    var mojo_tensor_sigmoided: Tensor[type] = nn_utils.sigmoid(mojo_tensor)
    
    var dur2 = time.time()-tm2
    print("Mojo Sigmoid:",dur2,"seconds")

    if mojo_tensor_equals_pytroch_tensor(mojo_tensor_sigmoided, pytorch_tensor) == True:
        tests_passed = True
                
    if tests_passed:
        print("✅ Passed")
    else:
        print("❌ Failed")
        
    print("--------------------------------------------------\n")

    assert_true(tests_passed)
    
def test_transpose():
    torch = Python.import_module("torch")
    nn = Python.import_module("torch.nn")
    np = Python.import_module("numpy")
    
    print("Running Transpose Tests...")
    print("--------------------------------------------------")
    
    var tests_passed: Bool = False
    
    # 2D Tensor Transpose
    var numpy_array_2D = np.random.uniform(-10, 10, size=(8,3))
    var mojo_tensor_2D = create_equivalent_mojo_tensor(numpy_array_2D)  
    var pytorch_tensor_2D: PythonObject = torch.tensor(numpy_array_2D)
    
    var nn_utils = NN_Utils()
    
    var mojo_tensor_transposed_2D = nn_utils.transpose(mojo_tensor_2D)
    pytorch_tensor_transposed_2D = pytorch_tensor_2D.transpose(-1, 0)
                
    if mojo_tensor_equals_pytroch_tensor(mojo_tensor_transposed_2D, pytorch_tensor_transposed_2D) == True:
        tests_passed = True
    
    if tests_passed:
        print("✅ Passed")
    else:
        print("❌ Failed")
        
    print("--------------------------------------------------\n")

    assert_true(tests_passed)
    
def test_matmul():
    torch = Python.import_module("torch")
    nn = Python.import_module("torch.nn")
    np = Python.import_module("numpy")
    
    print("Running Matmul Tests...")
    print("--------------------------------------------------")
        
    var tests_passed: Bool = False
    
    var numpy_array_2D_1 = np.random.uniform(-10, 10, size=(254,128))
    var numpy_array_2D_2 = np.random.uniform(-10, 10, size=(128,254))
    var mojo_tensor_2D_1 = create_equivalent_mojo_tensor(numpy_array_2D_1)  
    var mojo_tensor_2D_2 = create_equivalent_mojo_tensor(numpy_array_2D_2) 
    var pytorch_tensor_2D_1: PythonObject = torch.tensor(numpy_array_2D_1)
    var pytorch_tensor_2D_2: PythonObject = torch.tensor(numpy_array_2D_2)
    
    var nn_utils = NN_Utils()
    
    var time = Python.import_module("time")

    var tm1 = time.time()  
    pytorch_tensor_matmuled_2D = torch.matmul(pytorch_tensor_2D_1, pytorch_tensor_2D_2)
    var dur1 = time.time()-tm1
    print("Pytorch Sigmoid:",dur1,"seconds")
    
    var tm2 = time.time()
    var mojo_tensor_matmuled_2D = nn_utils.matmul_simple(mojo_tensor_2D_1, mojo_tensor_2D_2)
    var dur2 = time.time()-tm2
    print("Mojo Sigmoid:",dur2,"seconds")
                
    if mojo_tensor_equals_pytroch_tensor(mojo_tensor_matmuled_2D, pytorch_tensor_matmuled_2D) == True:
        tests_passed = True
        
    if tests_passed:
        print("✅ Passed")
    else:
        print("❌ Failed")
        
    print("--------------------------------------------------\n")

    assert_true(tests_passed)
    
def test_mojo_NN():
    var nn = Python.import_module("torch.nn")
    var optim = Python.import_module("torch.optim")
    var torch = Python.import_module("torch")  
    var time = Python.import_module("time")
    var F = Python.import_module("torch.nn.functional")
    
    var nn_utils = NN_Utils()
    var batchSize: Int = 64
    var learning_rate = 0.01
    var device: PythonObject = torch.device("cpu")
    
    print("Running Mojo NN Tests...")
    print("--------------------------------------------------")
      
    # Load MNIST dataset
    var trainloader = MNIST_Data_Loader_Pytorch(batchSize=batchSize, device=device, train=True, shuffle=True)

    var model: Custom_NN_Pytorch = Custom_NN_Pytorch()

    var criterion = nn.NLLLoss()  # Negative Log Likelihood Loss
    var optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    
    # Get Batch
    var images_pytorch = trainloader.batchImages[0].to(device)
    var labels_pytorch = trainloader.batchLabels[0].to(device)
    var images_mojo: Tensor[type] = create_equivalent_mojo_tensor(images_pytorch)
    var tensor_shape_list = List[Int]()
    tensor_shape_list.append(64)
    var labels_mojo = Tensor[DType.int64](TensorShape(tensor_shape_list))
    for i in range(labels_pytorch.shape[0]):
        labels_mojo[i] = Int(labels_pytorch[i])

    var tests_passed = False
    
    # Run tests
    
    # Zero out gradients
    optimizer.zero_grad()
    
    # Flatten
    var flat_pytorch: PythonObject = model.flatten(images_pytorch)        
    var flat_mojo = images_mojo
    flat_mojo.ireshape(TensorShape(batchSize, 28 * 28))
    
    if mojo_tensor_equals_pytroch_tensor(flat_mojo, flat_pytorch) == True:
        tests_passed = True
                
    # FC1
    var fc1_weight_pytorch = model.fc1.weight
    var fc1_weight_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc1_weight_pytorch.t())
    var fc1_bias_pytorch = model.fc1.bias
    var fc1_bias_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc1_bias_pytorch)
         
    var fc1_pytorch = model.fc1(flat_pytorch)
    var fc1_manual = torch.matmul(flat_pytorch, fc1_weight_pytorch.t()) + fc1_bias_pytorch
    # x @ weight
    var fc1_mojo = nn_utils.matmul_simple(flat_mojo, fc1_weight_mojo)
    # Add bias
    for i in range(fc1_mojo.shape()[0]):
        for j in range(fc1_mojo.shape()[1]):
            fc1_mojo[Index(i, j)] = fc1_mojo[Index(i, j)] + fc1_bias_mojo[Index(j)]
            
    if tests_passed and torch.allclose(fc1_pytorch, fc1_manual, atol=1e-4) and mojo_tensor_equals_pytroch_tensor(fc1_mojo, fc1_pytorch):
        tests_passed = True
    else:
        tests_passed = False
            
    # Relu FC1
    var relu_fc1_pytorch = F.relu(fc1_pytorch)
    var fc1_mojo_relued: Tensor[type] = nn_utils.relu(fc1_mojo)
    
    if tests_passed and mojo_tensor_equals_pytroch_tensor(fc1_mojo_relued, relu_fc1_pytorch):
        tests_passed = True
    else:
        tests_passed = False
        
    # FC2
    var fc2_weight_pytorch = model.fc2.weight
    var fc2_weight_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc2_weight_pytorch)
    var fc2_bias_pytorch = model.fc2.bias
    var fc2_bias_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc2_bias_pytorch)
     
    var fc2_pytorch = model.fc2(relu_fc1_pytorch)
    var fc2_manual = torch.matmul(relu_fc1_pytorch, fc2_weight_pytorch.t()) + fc2_bias_pytorch
    # x @ weight
    var fc2_mojo = nn_utils.matmul_simple(fc1_mojo_relued, nn_utils.transpose(fc2_weight_mojo))
    # Add bias
    for i in range(fc2_mojo.shape()[0]):
        for j in range(fc2_mojo.shape()[1]):
            fc2_mojo[Index(i, j)] = fc2_mojo[Index(i, j)] + fc2_bias_mojo[Index(j)]
            
    if tests_passed and torch.allclose(fc2_pytorch, fc2_manual, atol=1e-4) and mojo_tensor_equals_pytroch_tensor(fc2_mojo, fc2_pytorch):
        tests_passed = True
    else:
        tests_passed = False
    
    # Relu FC2
    var relu_fc2_pytorch = F.relu(fc2_pytorch)
    var fc2_mojo_relued: Tensor[type] = nn_utils.relu(fc2_mojo)
    
    if tests_passed and mojo_tensor_equals_pytroch_tensor(fc2_mojo_relued, relu_fc2_pytorch):
        tests_passed = True
    else:
        tests_passed = False
        
    # FC3
    var fc3_weight_pytorch = model.fc3.weight
    var fc3_weight_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc3_weight_pytorch)
    var fc3_bias_pytorch = model.fc3.bias
    var fc3_bias_mojo: Tensor[type] = create_equivalent_mojo_tensor(fc3_bias_pytorch)
     
    var fc3_pytorch = model.fc3(relu_fc2_pytorch)
    var fc3_manual = torch.matmul(relu_fc2_pytorch, fc3_weight_pytorch.t()) + fc3_bias_pytorch
    # x @ weight
    var fc3_mojo = nn_utils.matmul_simple(fc2_mojo_relued, nn_utils.transpose(fc3_weight_mojo))
    # Add bias
    for i in range(fc3_mojo.shape()[0]):
        for j in range(fc3_mojo.shape()[1]):
            fc3_mojo[Index(i, j)] = fc3_mojo[Index(i, j)] + fc3_bias_mojo[Index(j)]
            
    if tests_passed and torch.allclose(fc3_pytorch, fc3_manual, atol=1e-4) and mojo_tensor_equals_pytroch_tensor(fc3_mojo, fc3_pytorch):
        tests_passed = True
    else:
        tests_passed = False
        
    # Log Softmax
    var log_softmax_pytorch = F.log_softmax(fc3_pytorch, dim=1)
    var log_softmax_mojo = nn_utils.log_softmax(fc3_mojo, dim=1)
        
    if tests_passed and mojo_tensor_equals_pytroch_tensor(log_softmax_mojo, log_softmax_pytorch):
        tests_passed = True
    else:
        tests_passed = False
        
    # Negative Log Likelihood Loss
    var loss_pytorch = criterion(log_softmax_pytorch, labels_pytorch)
    var loss_mojo: Float64 = nn_utils.nll_loss(log_softmax_mojo, labels_mojo)
    
    if tests_passed and ((Float64(loss_pytorch.item()) > (loss_mojo - 0.0001)) or (Float64(loss_pytorch.item()) <= (loss_mojo + 0.0001))):
        tests_passed = True
    else:
        tests_passed = False
        
        
    # Backward
    loss_pytorch.backward()
    
    # Gradient of loss with respect to the output of fc3 (logits)
    logits = fc3_pytorch
    logits_mojo = fc3_mojo

    # Gradient of NLLLoss w.r.t logits
    softmax = torch.exp(log_softmax_pytorch)
    var softmax_mojo = Tensor[type](log_softmax_mojo.shape())
    for i in range(softmax_mojo.shape()[0]):
        for j in range(softmax_mojo.shape()[1]):
            softmax_mojo[Index(i,j)] = exp(log_softmax_mojo[Index(i,j)])
            
    grad_output = softmax.clone()
    for i in range(batchSize):
        grad_output[i, labels_pytorch[i]] -= 1
        
    var grad_output_mojo: Tensor[type] = softmax_mojo.copy()
    for i in range(batchSize):
        grad_output_mojo[Index(i, labels_mojo[i])] = (grad_output_mojo[Index(i, labels_mojo[i])] - 1) 
        
    # If loss was averaged, divide by batchSize
    grad_output /= batchSize
        
    for i in range(grad_output_mojo.shape()[0]):
        for j in range(grad_output_mojo.shape()[1]):
            grad_output_mojo[Index(i,j)] /= batchSize

    # This is the gradient of the loss w.r.t. logits
    # Gradient for the weights is the outer product of the log_softmax gradient and the input x
    weight_grad_manual = torch.matmul(relu_fc2_pytorch.T, grad_output).T
    var weight_grad_mojo = nn_utils.transpose(nn_utils.matmul_simple(nn_utils.transpose(fc2_mojo_relued), grad_output_mojo))
    
    # Now compute bias gradient: sum across samples (axis=0)
    bias_grad_manual = torch.sum(grad_output, dim=0)
    var bias_grad_mojo: Tensor[type] = Tensor[type](TensorShape(grad_output_mojo.shape()[1]))
    for i in range(bias_grad_mojo.shape()[0]):
        bias_grad_mojo[i] = 0.0
    for i in range(grad_output_mojo.shape()[0]):
        for j in range(grad_output_mojo.shape()[1]):
            bias_grad_mojo[j] += grad_output_mojo[Index(i, j)]
                
    if tests_passed and torch.allclose(model.fc3.weight.grad, weight_grad_manual, atol=1e-4) and torch.allclose(model.fc3.bias.grad, bias_grad_manual, atol=1e-4) \
    and mojo_tensor_equals_pytroch_tensor(weight_grad_mojo, model.fc3.weight.grad) and mojo_tensor_equals_pytroch_tensor(bias_grad_mojo, model.fc3.bias.grad):
        tests_passed = True
    else:
        tests_passed = False
        
    # relu(fc2)
    
    # grad of loss w.r.t. fc2 output
    grad_fc2_output_pytorch = torch.matmul(grad_output, model.fc3.weight) 
    var grad_fc2_output_mojo = nn_utils.matmul_simple(grad_output_mojo, fc3_weight_mojo)
    
    # backprop through ReLU after fc2
    relu_mask_fc2_pytorch = (fc2_pytorch > 0).float()
    grad_fc2_output_pytorch *= relu_mask_fc2_pytorch
    
    for i in range(grad_fc2_output_mojo.shape()[0]):
        for j in range(grad_fc2_output_mojo.shape()[1]):
            if ~(fc2_mojo[Index(i,j)] > 0.0):
                grad_fc2_output_mojo[Index(i,j)] = 0.0
                
    # grad w.r.t. fc2 weights
    weight_grad_fc2_manual_pytorch = torch.matmul(relu_fc1_pytorch.T, grad_fc2_output_pytorch).T
    var weight_grad_fc2_mojo = nn_utils.transpose(nn_utils.matmul_simple(nn_utils.transpose(fc1_mojo_relued), grad_fc2_output_mojo))
        
    bias_grad_fc2_manual_pytorch = torch.sum(grad_fc2_output_pytorch, dim=0)  
    var bias_grad_fc2_manual_mojo: Tensor[type] = Tensor[type](grad_fc2_output_mojo.shape()[0])
    
    for i in range(grad_fc2_output_mojo.shape()[1]):
        bias_grad_fc2_manual_mojo[i] = 0
        for j in range(grad_fc2_output_mojo.shape()[0]):
            bias_grad_fc2_manual_mojo[i] = bias_grad_fc2_manual_mojo[i] + grad_fc2_output_mojo[Index(j,i)]
    
    if tests_passed and model.fc2.weight.grad.shape == weight_grad_fc2_manual_pytorch.shape \
    and torch.allclose(model.fc2.weight.grad, weight_grad_fc2_manual_pytorch, atol=1e-4) and torch.allclose(model.fc2.bias.grad, bias_grad_fc2_manual_pytorch, atol=1e-4) \
    and mojo_tensor_equals_pytroch_tensor(weight_grad_fc2_mojo, model.fc2.weight.grad) and mojo_tensor_equals_pytroch_tensor(bias_grad_fc2_manual_mojo, model.fc2.bias.grad):
        tests_passed = True
    else:
        tests_passed = False
        
    # relu(fc1)
    grad_fc1_output_pytorch = torch.matmul(grad_fc2_output_pytorch, model.fc2.weight)
    var grad_fc1_output_mojo = nn_utils.matmul_simple(grad_fc2_output_mojo, fc2_weight_mojo)

    relu_mask_fc1_pytorch = (fc1_pytorch > 0).float()
    grad_fc1_output_pytorch *= relu_mask_fc1_pytorch

    for i in range(grad_fc1_output_mojo.shape()[0]):
        for j in range(grad_fc1_output_mojo.shape()[1]):
            if ~(fc1_mojo[Index(i,j)] > 0.0):
                grad_fc1_output_mojo[Index(i,j)] = 0.0
                     
    weight_grad_fc1_manual_pytorch = torch.matmul(flat_pytorch.T, grad_fc1_output_pytorch).T
    var weight_grad_fc1_mojo = nn_utils.transpose(nn_utils.matmul_simple(nn_utils.transpose(flat_mojo), grad_fc1_output_mojo))
    
    bias_grad_fc1_manual_pytorch = torch.sum(grad_fc1_output_pytorch, dim=0)
    var bias_grad_fc1_mojo: Tensor[type] = Tensor[type](grad_fc1_output_mojo.shape()[1])
    for i in range(grad_fc1_output_mojo.shape()[1]):
        bias_grad_fc1_mojo[i] = 0
        for j in range(grad_fc1_output_mojo.shape()[0]):
            bias_grad_fc1_mojo[i] += grad_fc1_output_mojo[Index(j,i)]

    if tests_passed and model.fc1.weight.grad.shape == weight_grad_fc1_manual_pytorch.shape \
    and torch.allclose(model.fc1.weight.grad, weight_grad_fc1_manual_pytorch, atol=1e-4) and torch.allclose(model.fc1.bias.grad, bias_grad_fc1_manual_pytorch, atol=1e-4) \
    and mojo_tensor_equals_pytroch_tensor(weight_grad_fc1_mojo, model.fc1.weight.grad) and mojo_tensor_equals_pytroch_tensor(bias_grad_fc1_mojo, model.fc1.bias.grad):
        tests_passed = True
    else:
        tests_passed = False
        
    if tests_passed:
        print("✅ Passed")
    else:
        print("❌ Failed")
    
    print("--------------------------------------------------\n")
    
    assert_true(tests_passed)
    
def run_tests():
    torch = Python.import_module("torch")
    nn = Python.import_module("torch.nn")
    
    print("\nRunning Tests...")
    print("==================================================\n")
    #test_sigmoid()
    #test_transpose()
    #test_matmul()
    test_mojo_NN()
        
    print("==================================================\n")
    
    
fn main() raises:
    seed(42)
    
    run_tests()  
    
    var mojo_tensor = Tensor[type](TensorShape(2,3,4,5))
    var mojo_tensor_sub = Tensor[type](TensorShape(5))
    
    #mojo_tensor[Index(0,0,0)] = mojo_tensor_sub