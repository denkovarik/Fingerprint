from python import Python, PythonObject
from tensor import Tensor, TensorSpec, TensorShape
from memory import UnsafePointer
from collections import Dict
from test_utils import create_equivalent_mojo_tensor
from NN_Utils import NN_Utils
from utils.index import Index, IndexList
from math import exp
from algorithm import vectorize, parallelize
from sys.info import simdwidthof, simdbitwidth


alias type = DType.float64
alias simd_width: Int = simdwidthof[type]()


@value
struct Custom_NN_Pytorch:
    var torch: PythonObject
    var nn: PythonObject
    var F: PythonObject
    var model_parameters: PythonObject
    var device: PythonObject
    var flatten: PythonObject
    var fc1: PythonObject

    fn __init__(mut self) raises:
        self.torch = Python.import_module("torch")
        self.nn = Python.import_module("torch.nn")
        self.F = Python.import_module("torch.nn.functional")
        self.model_parameters = Python.list()
        self.device = self.torch.device("cpu")
        self.flatten = self.nn.Flatten()
        self.fc1 = self.nn.Linear(28 * 28, 10) 
                
    def forward(mut self, x: PythonObject) -> PythonObject:
        pass
        var temp1: PythonObject = self.flatten(x)
        #var temp2: PythonObject = self.fc1(temp1)
        #var out: PythonObject = self.F.log_softmax(temp2, dim=1)
        return x
        #return out
        
    def parameters(mut self) -> PythonObject:
        self.model_parameters = Python.list()
        self.model_parameters.append(self.fc1.weight)
        self.model_parameters.append(self.fc1.bias)
        return self.model_parameters

    fn zero_grad(mut self) raises:
        for param in self.model_parameters:
            try:
                param.grad.zero_()
            except AttributeError:
                pass
                

@value        
struct Custom_NN_Mojo:
    var batch_size: Float64
    var fc1_weight_mojo: Tensor[type]
    var fc1_bias_mojo: Tensor[type]
    var learning_rate: Float64
    var bias_grad_fc1_mojo: Tensor[type]
    var weight_grad_fc1_mojo: Tensor[type]
    var log_softmax_mojo: Tensor[type]   
    var fc1_mojo: Tensor[type]
    var flat_mojo: Tensor[type]
    var labels: Tensor[DType.int64]
    
    fn __init__(mut self) raises:
        var model: Custom_NN_Pytorch = Custom_NN_Pytorch()
        self.learning_rate = 0.01
        self.batch_size = 1
        
        # FC1
        var fc1_weight_pytorch = model.fc1.weight.clone()
        self.fc1_weight_mojo = create_equivalent_mojo_tensor(fc1_weight_pytorch)
        var fc1_bias_pytorch = model.fc1.bias.clone()
        self.fc1_bias_mojo = create_equivalent_mojo_tensor(fc1_bias_pytorch)
        
        self.bias_grad_fc1_mojo = Tensor[type](TensorShape(10, 64))
        self.weight_grad_fc1_mojo = Tensor[type](TensorShape(10, 64))
        self.log_softmax_mojo = Tensor[type](TensorShape(10, 64))
        self.fc1_mojo = Tensor[type](TensorShape(10, 64))
        self.labels = Tensor[DType.int64](TensorShape(10, 64))
        self.flat_mojo = Tensor[type](TensorShape(64, 1, 28, 28))
    
    
    fn forward(mut self, mut images: Tensor[type]) raises:
        var nn_utils = NN_Utils()
    
        self.batch_size = images.shape()[0]
       
        # Flatten
        var flat_mojo = images.ireshape(TensorShape(images.shape()[0], 28 * 28))
            
        # Foward: FC1
        var trans = nn_utils.transpose(self.fc1_weight_mojo)
        #self.fc1_mojo = nn_utils.matmul(self.flat_mojo, nn_utils.transpose(self.fc1_weight_mojo))
        #self.fc1_mojo = nn_utils.matmul_v3(self.flat_mojo, self.fc1_weight_mojo)
        # Add bias                
        #for i in range(self.fc1_mojo.shape()[0]):
        #    for j in range(self.fc1_mojo.shape()[1]):
        #        self.fc1_mojo[Index(i, j)] = self.fc1_mojo[Index(i, j)] + self.fc1_bias_mojo[Index(j)]
            
        # Forward: Log Softmax
        #self.log_softmax_mojo = nn_utils.log_softmax(self.fc1_mojo, dim=1)
       
        #return images
        #return self.log_softmax_mojo 
        
    fn loss(mut self, labels: Tensor[DType.int64]) -> Float64:
        self.labels = labels
        var nn_utils = NN_Utils()
        # Negative Log Likelihood Loss
        var loss_mojo: Float64 = nn_utils.nll_loss(self.log_softmax_mojo, self.labels)
        return loss_mojo
        
    fn backward(mut self):
        var nn_utils = NN_Utils()
        
        # Gradient of NLLLoss w.r.t logits
        var grad_output_mojo = Tensor[type](self.log_softmax_mojo.shape())
        for i in range(self.batch_size):
            for j in range(grad_output_mojo.shape()[1]):
                grad_output_mojo[Index(i,j)] = exp(self.log_softmax_mojo[Index(i,j)])
            grad_output_mojo[Index(i, self.labels[Index(i)])] = (grad_output_mojo[Index(i, self.labels[Index(i)])] - 1) 
            
        # If loss was averaged, divide by batch_size               
        for i in range(grad_output_mojo.shape()[0]):
            for j in range(grad_output_mojo.shape()[1]):
                grad_output_mojo[Index(i,j)] /= self.batch_size
            
        #print(grad_output_mojo.shape())
        #print(self.flat_mojo.shape())
        #print(nn_utils.transpose(self.flat_mojo).shape())
        #print(grad_output_mojo.shape())
        self.weight_grad_fc1_mojo = nn_utils.transpose(nn_utils.matmul(nn_utils.transpose(self.flat_mojo), grad_output_mojo))
        #self.weight_grad_fc1_mojo = nn_utils.transpose(nn_utils.matmul_v3(nn_utils.transpose(self.flat_mojo), nn_utils.transpose(grad_output_mojo)))
        #print(self.weight_grad_fc1_mojo.shape())
        #print("")
        
        self.bias_grad_fc1_mojo = Tensor[type](grad_output_mojo.shape()[1])
        for i in range(grad_output_mojo.shape()[1]):
            self.bias_grad_fc1_mojo[i] = 0.0
            for j in range(grad_output_mojo.shape()[0]):
                self.bias_grad_fc1_mojo[i] += grad_output_mojo[Index(j,i)]
                
    fn step(mut self):    
        # FC1 weight update
        for i in range(self.fc1_weight_mojo.shape()[0]):
            for j in range(self.fc1_weight_mojo.shape()[1]):
                self.fc1_weight_mojo[Index(i,j)] = self.fc1_weight_mojo[Index(i,j)] - self.learning_rate * self.weight_grad_fc1_mojo[Index(i,j)]
        # FC1 bias update
        for i in range(self.fc1_bias_mojo.shape()[0]):
            self.fc1_bias_mojo[i] = self.fc1_bias_mojo[i] - self.learning_rate * self.bias_grad_fc1_mojo[i]
            